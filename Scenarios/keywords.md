# ðŸŽ¯ CIS 6217 â€“ Comprehensive Concept Keywords (Lectures 1â€“13)

> **Description:**  
> This document consolidates the **core concepts, key terms, and fundamental ideas**
> covered across Lectures 1â€“13 of the CIS 6217 course.
>
> The keywords are intended to:
> - Support precise academic explanations
> - Help structure written answers clearly
> - Ensure consistency with lecture terminology
> - Provide a compact reference of essential concepts

---

## ðŸ”‘ Core Concept Keywords

- Model performance â†’ **Accuracy is not enough**
- CNN advantage â†’ **Spatial structure**
- Overfitting meaning â†’ **Fits noise**
- GAN failure â†’ **Training imbalance**
- LSTM necessity â†’ **Temporal dependency**
- Autoencoder goal â†’ **Representation learning**
- Feature visualization â†’ **Interpretability**
- Adversarial weakness â†’ **Non-robust features**
- Scene understanding â†’ **Object relationships**
- Tracking purpose â†’ **Temporal consistency**

---

## ðŸ§  Fundamental Topics

- Image formation â†’ **Perspective projection**
- Classification loss â†’ **Cross-Entropy**
- CNN kernel â†’ **Local receptive field**
- Pooling â†’ **Translation invariance**
- Overfitting sign â†’ **Training â†“ / Validation â†‘**
- Dropout â†’ **Prevent co-adaptation**
- Data augmentation â†’ **Artificial diversity**
- GAN objective â†’ **Minimax zero-sum game**
- Generator limitation â†’ **No useful gradients**
- CNN vs LSTM â†’ **Spatial vs temporal modeling**

---

## ðŸ“˜ Supporting Concepts

- KNN weakness â†’ **Slow inference**
- Generalization â†’ **Unseen data**
- Gradient descent â†’ **Optimization**
- Learning rate issue â†’ **Oscillation**
- Adam optimizer â†’ **Adaptive learning rate**
- VGG design â†’ **3Ã—3 kernels**
- ResNet idea â†’ **Skip connections**
- Denoising autoencoder â†’ **Robust features**
- Detection output â†’ **Bounding box + score**
- Optical flow â†’ **Pixel displacement**

---

## ðŸ“‚ Extended Terminology

- Hand-crafted features â†’ **HoG / SIFT**
- CNN feature map â†’ **Filter response**
- Padding â†’ **Preserve spatial size**
- Stride â†’ **Downsampling**
- Regularization â†’ **Penalty on weights**
- Early stopping â†’ **Best validation**
- Mode collapse â†’ **Low diversity**
- Latent space â†’ **Compressed representation**
- Instance segmentation â†’ **Separate objects**
- Scene graph â†’ **Graph structure**

---

## ex
- **Why CNN > MLP?**  
  â†’ *Uses spatial structure via local receptive fields and weight sharing.*

- **Why LSTM for video?**  
  â†’ *Models temporal dependencies across frames.*

- **Why GAN unstable?**  
  â†’ *Imbalance between Generator and Discriminator.*

- **Why Dropout works?**  
  â†’ *Prevents co-adaptation of neurons.*

- **Why Autoencoder useful?**  
  â†’ *Learns compact latent representations.*

---
