# Integrated Scenario (Computer Vision)

## Scenario

You are part of a research team developing an **intelligent computer vision system**
for a **smart city monitoring platform**.

The system processes:
- **Static images** from surveillance cameras
- **Video streams** from traffic intersections

The goal is to:
- Understand scenes
- Detect and classify objects
- Analyze motion and activities
- Ensure robust and explainable model behavior

The system combines multiple techniques including:
**CNNs, Autoencoders, GANs, LSTMs, Object Detection, Segmentation, Scene Graphs,
and Optimization methods**.

Answer the following questions based on this scenario.

---

## Question 1 – Image Formation

Objects that are far from the camera appear smaller, and parallel road lines converge
towards a single point.

**Question:**  
Which concept from **Image Formation** explains this phenomenon, and why is it
important in computer vision systems?

<details>
<summary><strong>Hidden Answer</strong></summary>

This is explained by **Perspective Projection** in the geometry of image formation.
It describes how 3D scenes are projected onto a 2D image plane. Ignoring it leads to
errors in depth estimation and object localization.
</details>

---

## Question 2 – CNN vs Traditional Methods

Initially, a **KNN classifier** was tested for image classification, but prediction time
was extremely slow for large datasets.

**Question:**  
Why is KNN inefficient in this scenario, and why is a **CNN** a better alternative?

<details>
<summary><strong>Hidden Answer</strong></summary>

KNN is a non-parametric method that compares each test sample with all training samples,
leading to high computational cost. CNNs learn hierarchical features and perform fast
inference after training.
</details>

---

## Question 3 – Loss Function Selection

The system performs **multi-class image classification**.

**Question:**  
Which **loss function** is most appropriate, and why is **Mean Squared Error (MSE)**
not suitable?

<details>
<summary><strong>Hidden Answer</strong></summary>

Cross-Entropy Loss is appropriate because it models class probabilities.
MSE is designed for regression and does not reflect probabilistic classification well.
</details>

---

## Question 4 – Overfitting Diagnosis

During training, **training loss decreases steadily**, while **validation loss increases**.

**Question:**  
What problem is occurring, and mention two techniques to address it.

<details>
<summary><strong>Hidden Answer</strong></summary>

The problem is **overfitting**. Possible solutions include:
Data Augmentation, Dropout, Early Stopping, or L2 Regularization.
</details>

---

## Question 5 – CNN Architecture Design

The model uses multiple **3×3 convolution kernels** instead of a single large kernel.

**Question:**  
Why is this design choice preferred, as seen in architectures like **VGGNet**?

<details>
<summary><strong>Hidden Answer</strong></summary>

Smaller kernels reduce the number of parameters, increase non-linearity,
improve feature learning, and reduce overfitting.
</details>

---

## Question 6 – Optimization Behavior

A high **learning rate** causes unstable loss curves during training.

**Question:**  
Explain why this happens and name an optimizer that can help.

<details>
<summary><strong>Hidden Answer</strong></summary>

A high learning rate overshoots the minimum, causing oscillations.
Optimizers like Adam or SGD with Momentum help stabilize convergence.
</details>

---

## Question 7 – GAN Training Dynamics

While training a **GAN**, the Discriminator reaches near-perfect accuracy early.

**Question:**  
What effect does this have on the Generator?

<details>
<summary><strong>Hidden Answer</strong></summary>

A very strong Discriminator provides weak gradients to the Generator,
preventing effective learning and causing training collapse.
</details>

---

## Question 8 – Minimax Game Concept

GAN training is described as a **minimax zero-sum game**.

**Question:**  
Explain this concept in words without mathematical formulation.

<details>
<summary><strong>Hidden Answer</strong></summary>

The Generator tries to fool the Discriminator, while the Discriminator tries
to correctly distinguish real from fake samples. Each network improves at
the expense of the other.
</details>

---

## Question 9 – Video Understanding

For **activity recognition in videos**, CNNs are combined with **LSTMs**.

**Question:**  
Why are CNNs alone insufficient, and what role does LSTM play?

<details>
<summary><strong>Hidden Answer</strong></summary>

CNNs capture spatial features only. LSTMs model temporal dependencies across frames,
which is essential for understanding actions in videos.
</details>

---

## Question 10 – Representation Learning

To improve robustness and reduce noise, **Autoencoders** are added to the pipeline.

**Question:**  
How do **Denoising Autoencoders** improve the system?

<details>
<summary><strong>Hidden Answer</strong></summary>

They learn to reconstruct clean inputs from corrupted data, forcing the model
to capture essential and robust features.
</details>

---

## Bonus Question – Model Interpretability

The team wants to explain model decisions using **Feature Visualization** and
**Network Inversion**.

**Question:**  
What insight do these techniques provide about CNN layers?

<details>
<summary><strong>Hidden Answer</strong></summary>

Early layers capture edges and textures, while deeper layers encode abstract
and semantic information such as object parts and categories.
</details>
